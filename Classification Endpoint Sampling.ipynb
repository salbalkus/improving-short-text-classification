{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343f8d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from icego import *\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from time import time\n",
    "\n",
    "# Need an OpenAI API key in a .env file to call GPT-3\n",
    "env_path = r'.env'\n",
    "dotenv_path = Path(env_path)\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "#openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r\"saved_models\\mar23_2.tsv\",sep=\"\\t\",index_col=0)\n",
    "df2 = pd.read_csv(r\"saved_models\\mar24.tsv\",sep=\"\\t\",index_col=0)\n",
    "df3 = pd.read_csv(r\"saved_models\\mar27.tsv\",sep=\"\\t\",index_col=0)\n",
    "df = pd.concat([df1, df2, df3]).reset_index(drop=True)\n",
    "\n",
    "training = pd.read_csv(r\"research_data\\training(1).tsv\", sep=\"\\t\", index_col=0)\n",
    "validation = pd.read_csv(r\"research_data\\validation(2).tsv\", sep=\"\\t\", index_col=0)\n",
    "testing = pd.read_csv(r\"research_data\\testing(3).tsv\", sep=\"\\t\", index_col=0)\n",
    "\n",
    "# Train-test split\n",
    "X_train = training[\"question\"]\n",
    "y_train = training[\"label\"]\n",
    "\n",
    "X_val = validation[\"question\"]\n",
    "y_val = validation[\"label\"]\n",
    "\n",
    "X_test = testing[\"question\"]\n",
    "y_test = testing[\"label\"]\n",
    "\n",
    "# Prep training for sending to GPT-3\n",
    "training[\"text\"] = training[\"question\"]\n",
    "training = training.drop(\"question\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d904c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(label):\n",
    "    if label == \"Topic: Data\":\n",
    "        return \"Data\"\n",
    "    elif label == \"Topic: Other\":\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        raise Exception(\"Class not Data or Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = pd.Series(dtype=\"object\")\n",
    "ys = pd.Series(dtype=\"object\")\n",
    "for i in range(max(df.index)):\n",
    "    entry = df.context[i].split(\"\\n\")[1:-1]\n",
    "    X = pd.Series(entry[::2])\n",
    "    y = pd.Series(entry[1::2]).map(relabel)\n",
    "    Xs = pd.concat([Xs, X])\n",
    "    ys = pd.concat([ys, y])\n",
    "Xs = Xs.reset_index(drop=True)\n",
    "ys = ys.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(data={\"text\":Xs, \"label\":ys}).drop_duplicates().reset_index(drop=True)\n",
    "df_new = df_new[df_new.text != \"\"]\n",
    "\n",
    "orig = pd.read_csv(\"initial_qs.csv\", index_col=0)\n",
    "df_new = pd.concat([df_new, orig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e65f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT3Classifier():\n",
    "    def __init__(self, file, max_examples=5, temperature=0, search_model=\"ada\", model=\"ada\"):\n",
    "        self.file = file\n",
    "        self.max_examples = max_examples\n",
    "        self.temperature = temperature\n",
    "        self.search_model = search_model\n",
    "        self.model = model\n",
    "        \n",
    "\n",
    "    def predict(self, text):\n",
    "        output = openai.Classification.create(\n",
    "            file=self.file,\n",
    "            query=text,\n",
    "            search_model=self.search_model, \n",
    "            model=self.model, \n",
    "            max_examples=self.max_examples,\n",
    "            temperature = self.temperature\n",
    "        )\n",
    "        return output\n",
    "    \n",
    "    def evaluate(self, X):\n",
    "        self.predictions = []\n",
    "        self.full_output = []\n",
    "        for val in X:\n",
    "            print(\"Evaluating: \" + val)\n",
    "            \n",
    "            # Sometimes the search may not find any similar documents.\n",
    "            # If this is the case, we skip it\n",
    "            result = None\n",
    "            try:\n",
    "                result = self.predict(val)\n",
    "            except:\n",
    "                print(\"Could not classify: \" + val)\n",
    "                self.full_output.append(None)\n",
    "                self.predictions.append('Unknown')\n",
    "                continue\n",
    "            self.full_output.append(result)\n",
    "            self.predictions.append(result['label'])\n",
    "        return self.predictions\n",
    "    \n",
    "    def accuracy(self, true):\n",
    "        return sum(pd.Series(true) == pd.Series(self.predictions)) / len(self.predictions)\n",
    "    \n",
    "    def save(self, path):\n",
    "        pickle_out = open(path, \"wb\")\n",
    "        pickle.dump(self, pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def load(env, model):\n",
    "        \"\"\"\n",
    "        Load model from a pickle file\n",
    "        \"\"\"\n",
    "\n",
    "        # Load API Key\n",
    "        try:\n",
    "            dotenv_path = Path(env)\n",
    "            load_dotenv(dotenv_path=dotenv_path)\n",
    "            openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        except:\n",
    "            raise Exception(\"Please specify your OpenAI API key in a .env file as OPENAI_API_KEY=your_key, and provide in the \\\"path\\\" parameter\")\n",
    "        pickle_in = open(model, \"rb\")\n",
    "        return pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08acad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization parameters using grid search\n",
    "def grid_search(file, X_test, y_test, examples, temperatures):\n",
    "    times = []\n",
    "    accuracies = []\n",
    "\n",
    "    for example in examples:\n",
    "        times_ex = []\n",
    "        accuracies_ex = []\n",
    "        for temperature in temperatures:\n",
    "            print(\"({}, {})\".format(temperature, example))\n",
    "            model25 = GPT3Classifier(file, max_examples=int(example), temperature=temperature)\n",
    "            start = time()\n",
    "            model25.evaluate(X_test)\n",
    "            time_taken = time() - start\n",
    "            acc = model25.accuracy(y_test)\n",
    "\n",
    "            times_ex.append(time_taken)\n",
    "            accuracies_ex.append(acc)\n",
    "\n",
    "        times.append(times_ex)\n",
    "        accuracies.append(accuracies_ex)\n",
    "        \n",
    "    return pd.DataFrame(accuracies, columns=temperatures, index=examples), pd.DataFrame(times, columns=temperatures, index=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_endpoint(n, df_sample, training, examples, temps, X_val, y_val, X_test, y_test, iters=5):\n",
    "    test_errs = []\n",
    "    for i in range(iters):\n",
    "        print(\"Iteration {}\".format(i))\n",
    "        df = df_sample.sample(n=n)\n",
    "        df = pd.concat([df, training]).reset_index(drop=True)\n",
    "        df.to_json(r\"saved_models\\gpt3_qs_{}_{}.jsonl\".format(n,i),orient=\"records\",lines=True)\n",
    "        result = openai.File.create(file=open(r\"saved_models\\gpt3_qs_{}_{}.jsonl\".format(n,i)), purpose=\"classifications\")\n",
    "\n",
    "        acc, runtime = grid_search(result['id'], X_val, y_val, examples, temps)\n",
    "        acc.to_csv(r'saved_models\\classifier_acc{}_{}_mar28.csv'.format(n,i))\n",
    "        runtime.to_csv(r'saved_models\\classifier_time{}_{}_mar28.csv'.format(n,i))\n",
    "        \n",
    "        best_temp = acc.max().idxmax()\n",
    "        best_n_exs = acc[best_temp].idxmax()\n",
    "        best_col = acc[acc.max().idxmax()].idxmax()\n",
    "        \n",
    "        best_classifier = GPT3Classifier(result['id'], max_examples=int(best_n_exs), temperature=float(best_temp))\n",
    "        best_classifier.evaluate(X_test)\n",
    "        best_classifier.save(r'saved_models\\best{}_{}_mar28.p'.format(n, i))\n",
    "        \n",
    "        test_errs.append(best_classifier.accuracy(y_test))\n",
    "       \n",
    "    # Save test error output\n",
    "    pickle_out = open(r'saved_models\\best{}_{}_testerrs_mar28.p'.format(n, i), \"wb\")\n",
    "    pickle.dump(test_errs, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22223d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "test_errs = []\n",
    "for i in range(5):\n",
    "    best_classifier = pd.read_pickle(r'saved_models\\best{}_{}_mar28.p'.format(n,i))\n",
    "    best_classifier.temperature = float(best_classifier.temperature)\n",
    "    best_classifier.max_examples = int(best_classifier.max_examples)\n",
    "    best_classifier.evaluate(X_test)\n",
    "    best_classifier.save(r'saved_models\\best{}_{}_mar28_.p'.format(n, i))\n",
    "\n",
    "    test_errs.append(best_classifier.accuracy(y_test))\n",
    "\n",
    "pickle_out = open(r'saved_models\\best{}_{}_testerrs_mar28.p'.format(n, i), \"wb\")\n",
    "pickle.dump(test_errs, pickle_out)\n",
    "pickle_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2782346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60247082",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = np.array(list(range(1,6))) * 5\n",
    "temperatures = [0, 0.1, 0.5]\n",
    "evaluate_classification_endpoint(10, df_new, training, examples, temperatures, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(r'saved_models\\best10_4_testerrs_mar28.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = np.array(list(range(1,6))) * 5\n",
    "temperatures = [0, 0.1, 0.5]\n",
    "evaluate_classification_endpoint(100, df_new, training, examples, temperatures, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "test100 = pd.read_pickle(r'saved_models\\best100_4_testerrs_mar28.p')\n",
    "np.mean(test100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b121a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test10 = pd.read_pickle(r'saved_models\\best10_4_testerrs_mar28.p')\n",
    "np.mean(test10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = np.array(list(range(1,6))) * 5\n",
    "examples = examples.tolist()\n",
    "examples.append(100)\n",
    "temperatures = [0, 0.1, 0.5]\n",
    "evaluate_classification_endpoint(1000, df_new, training, examples, temperatures, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1000 = pd.read_pickle(r'saved_models\\best1000_4_testerrs_mar28.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification_endpoint(10000, df_new, training, examples, temperatures, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test10000 = pd.read_pickle(r'saved_models\\best10000_4_testerrs_mar28.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e983e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(test10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2723d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(test1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = np.array(list(range(1,6))) * 5\n",
    "examples = examples.tolist()\n",
    "examples.append(100)\n",
    "temperatures = [0, 0.1, 0.5]\n",
    "evaluate_classification_endpoint(0, df_new, training, examples, temperatures, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "testerrs = []\n",
    "valerrs = []\n",
    "\n",
    "for i in [0, 10,100,1000,10000]:\n",
    "    testerrs.append(pd.read_pickle(r'saved_models\\best{}_4_testerrs_mar28.p'.format(i)))\n",
    "\n",
    "    valerr = []\n",
    "    for j in range(5):\n",
    "        # Get the maximum accuracy produced by any given set of hyperparameters\n",
    "        valerr.append(pd.read_csv(r'saved_models\\classifier_acc{}_{}_mar28.csv'.format(i,j), index_col=0).to_numpy().max())\n",
    "    valerrs.append(valerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e71450",
   "metadata": {},
   "outputs": [],
   "source": [
    "testerrs = np.array(testerrs)\n",
    "testerrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "testerrs.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testerrs.std(axis=1) / np.sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4167044",
   "metadata": {},
   "outputs": [],
   "source": [
    "valerrs = np.array(valerrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valerrs.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "valerrs.std(axis=1) / np.sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcedde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import permutation_test, ttest_ind\n",
    "\n",
    "def statistic(x, y):\n",
    "    return np.mean(x) - np.mean(y)\n",
    "\n",
    "print(\"Test Error P-values\")\n",
    "for i in range(1,5):\n",
    "    res = permutation_test([testerrs[0], testerrs[i]], statistic, alternative = \"less\")\n",
    "    print(res.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import permutation_test, ttest_ind\n",
    "\n",
    "def statistic(x, y):\n",
    "    return np.mean(x) - np.mean(y)\n",
    "\n",
    "print(\"Validation Error P-values\")\n",
    "for i in range(1,5):\n",
    "    res = permutation_test([valerrs[0], valerrs[i]], statistic, alternative='less')\n",
    "    print(res.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37dbe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old data format\n",
    "exs = [0, 1, 2, 3, 4]\n",
    "valse = valerrs.std(axis=1) / np.sqrt(5)\n",
    "testse = testerrs.std(axis=1) / np.sqrt(5)\n",
    "overall = pd.DataFrame({\"exs\":exs, \"val\":valerrs.mean(axis=1), \"test\":testerrs.mean(axis=1), \"valse\":valse, \"testse\":testse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0484154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data format\n",
    "acc_test = testerrs.flatten()\n",
    "acc_val = valerrs.flatten()\n",
    "labs = np.array([[0]*5] + [[i]*5 for i in range(1,5)]).flatten()\n",
    "overall = pd.concat([pd.DataFrame({\"label\":labs, \"value\":acc_val, \"set\":\"Validation Set\"}), pd.DataFrame({\"label\":labs, \"value\":acc_test, \"set\":\"Test Set (Unseen)\"})], axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00633e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a25cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New plotting\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 2, style=\"white\", rc={'figure.figsize':(11,8)})\n",
    "p = sns.lineplot(data=overall, y=\"value\", x=\"label\", hue=\"set\", ci=68, marker='o', linewidth=2, markersize=10);\n",
    "p.set_ylabel(\"Accuracy\");\n",
    "p.set_xlabel(\"Number of New Example Questions Added\");\n",
    "p.legend(['Validation Set Mean', 'Validation Set SE','Test Set (Unseen) Mean','Test Set SE']);\n",
    "plt.xticks([0,1,2,3,4])\n",
    "p.set_xticklabels([\"0\",\"10\",\"100\",\"1,000\",\"10,000\"]);\n",
    "plt.tick_params(bottom=True, left=True)\n",
    "#plt.savefig('class_endpoint_simplified2.png', dpi=2000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897a613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New plotting\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 2, style=\"white\", rc={'figure.figsize':(11,8)})\n",
    "p = sns.lineplot(data=overall[overall.set==\"Validation Set\"], y=\"value\", x=\"label\", ci=68, marker='o', linewidth=2, markersize=10);\n",
    "p.set_ylabel(\"Accuracy\");\n",
    "p.set_xlabel(\"Number of New Example Questions Added\");\n",
    "p.legend(['Mean Accuracy','Standard Error']);\n",
    "plt.xticks([0,1,2,3,4])\n",
    "p.set_xticklabels([\"0\",\"10\",\"100\",\"1,000\",\"10,000\"]);\n",
    "plt.tick_params(bottom=True, left=True)\n",
    "plt.savefig('class_endpoint_simplified4.png', dpi=2000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5522ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New plotting\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 2, style=\"white\", rc={'figure.figsize':(11,8)})\n",
    "p = sns.lineplot(data=overall[overall.set==\"Test Set (Unseen)\"], y=\"value\", x=\"label\", ci=68, marker='o', linewidth=2, markersize=10);\n",
    "p.set_ylabel(\"Accuracy on Unseen Examples\");\n",
    "p.set_xlabel(\"Number of New Example Questions Added\");\n",
    "p.legend(['Mean Accuracy','Standard Error']);\n",
    "plt.xticks([0,1,2,3,4])\n",
    "p.set_xticklabels([\"0\",\"10\",\"100\",\"1,000\",\"10,000\"]);\n",
    "plt.tick_params(bottom=True, left=True)\n",
    "plt.savefig('class_endpoint_simplified3.png', dpi=2000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cff36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old plotting\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 1.25, style=\"white\")\n",
    "\n",
    "p = sns.lineplot(data=overall, y=\"val\",x=\"exs\", ci=\"valse\", err_style = \"band\", markers=True);\n",
    "p = sns.lineplot(data=overall, y=\"test\",x=\"exs\", ci=\"testse\", err_style = \"band\", markers=True);\n",
    "p.set_ylabel(\"Accuracy\");\n",
    "p.set_xlabel(\"Number of New Example Questions Added\");\n",
    "plt.xticks([0,1,2,3,4])\n",
    "p.set_xticklabels([\"0\",\"10\",\"100\",\"1,000\",\"10,000\"])\n",
    "p.fill_between(overall.exs, overall.val - overall.valse, overall.val + overall.valse, color='blue', alpha=0.2);\n",
    "p.fill_between(overall.exs, overall.test - overall.testse, overall.test + overall.testse, color='orange', alpha=0.2);\n",
    "p.legend(['Validation Set', 'Test Set (Unseen)'])\n",
    "\n",
    "plt.savefig('class_endpoint_simplified.png', dpi=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT3Classifier.load(env_path, r'saved_models\\best100_4_testerrs_mar28.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GPT3Classifier(\"file-QU5FZXAKFre6i1BblZdxdWzW\", max_examples=25, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classifier.evaluate(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88026718",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pd.DataFrame({\"Question\": X_test, \"true\": y_test,\"pred\": results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df99dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conf[conf.true != conf.pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.Question.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[[2,3,7,9,16,17,19]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d37bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time test\n",
    "time = []\n",
    "\n",
    "for i in [0, 10,100,1000,10000]:\n",
    "    tmp = []\n",
    "    for j in range(5):\n",
    "        # Sum the total time to run each set of hyperparameters\n",
    "        tmp.append(pd.read_csv(r'saved_models\\classifier_time{}_{}_mar28.csv'.format(i,j), index_col=0).to_numpy().sum())\n",
    "    time.append(tmp)\n",
    "time = np.array(time) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b94ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get average time for each number of new questions added\n",
    "time.mean(axis=1).std() / np.sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c066f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average time for each number of new questions added\n",
    "time.std(axis=1) / np.sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print('Sysinfo')\n",
    "uname = platform.uname()\n",
    "print(f\"System: {uname.system}\")\n",
    "print(f\"Node Name: {uname.node}\")\n",
    "print(f\"Release: {uname.release}\")\n",
    "print(f\"Version: {uname.version}\")\n",
    "print(f\"Machine: {uname.machine}\")\n",
    "print(f\"Processor: {uname.processor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd57a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std([105, 119, 112]) / np.sqrt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3022ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
